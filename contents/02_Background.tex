\emph{This chapter lays out the theoretical background of this thesis by first introducing the general concepts of a high-level language virtual machine (HLL-VM) like the Java Virtual Machine (JVM) and then going on to more specific features of the GraalVM. In the following the next abstraction layer the Truffle API is explained before coming to the explicit project Graal.Js which uses Truffle. The last section evolves around the ECMAScript parts of the topic with a brief glance at the current state of modules and what the proposal tries to achieve.}

\section{HLL-VMs}
\subsection{HLL-VM groundwork}
Generally speaking a high-level language virtual machine is an abstraction layer relieving the programmer from several tasks with the main feature being platform independent development. When starting to discuss the platform independence it is foremost important to note why programs are usually platform bound.\\
Every computer employs some kind of an instruction set architecture (ISA) and an operating system (OS). Every developed program is bound to these two technologies. If a program is developed for a particular pair of ISA/OS it has to be ported to run on a machine with a different pair of ISA/OS. The problem arising with that is huge support overhead since now every ported version has to receive different kinds of updates. It is thus highly impractical to enforce such a development environment for every application program. By developing a high-level language virtual machine this task is posed upon the development of the VM only and all other application programs being developped don't have to focus on platform dependency resulting in a more lean development process. How is the VM delivering the abstraction layer?\\
High-level language virtual machines enhanced the concept of early VMs like P-code by using virtual instruction set architectures (V-ISA) encompassing code and metadata, like data structures and resource-related information, independently of platforms. The code is simpley interpreted the metadata loaded and thus turned into a machine-dependent version by the virtual machines provided emulator. This alone already stands out as a huge accomplishment but HLL-VMs come with even more features.\\
In today's computer landscape the highest risk comes from untrusted software run on machines. The HLL-VM, especially the JVM,  provides a metaphoric sandbox in which the untrusted application can run without making the rest of the system outside of the VM vulnerable. But although it amplifies security there are loopholes to bypass said security measures especially when untrusted software is given explicit permission to go outside of the VM provided resources. So running untrusted software is still not advised but made more secure so with a HLL-VM. As said before security is not the only feature HLL-VMs serve. Making code robust is another one.\\
Especially when it comes to large-scale software systems robust software is key. Here the good fit of the object-oriented model and the platform independence make HLL-VMs the top technology in the field. How much a HLL-VM supports robustness is of course depending on the used VM but on the example of the JVM strong type-checking and garbage collection which will be explained later lift a lot of beverage off the programmer's shoulder making the programmer concentrate on the mere implementation and thus making the programmer produce more robust code.\\
Other merits of HLL-VMs come from technologies like dynamic linking saving network bandwidth or profiling for performance.\cite{Smith}\\
As the start of this subsection already stated a HLL-VM is an abstraction layer with a lot of features making a programmer's life considerably easier. This comes with some costs especially with performance. This cost is then highly reduced by certain techniques like the aforementioned profiling. With the groundwork laid out the next section discusses certain features of the JVM.

\subsection{The JVM}
The Java Virtual Machine is as its name states developed around the language Java. Java is a general-purpose, object-oriented language with strong static types aimed as a production language. Since one language target is simplicity details of machine representation are omitted and not accessible through the language. Further safety measures have been made like automatic storage management and checked array access. The Java sourcecode is usually compiled ahead of time (AOT) to the Java bytecode which is then run on the JVM. Ahead of time compilation means that the programmed sourcecode gets compiled into a machine specific executable form. This form is in the case of Java Java bytecode, i.e. a class-file.\cite{Gosling}\par As stated before the JVM is the virtual sandbox environment in which an AOT-compiled java program, represented by the java bytecode, is executed. The JVM, like an actual machine, has an instruction set and access to various memory areas. With this in mind a JVM can also be directly implemented as a CPU or in various other direct ways. From knowing what the JVM is, the next paragraph dives deeper into the topic and explains how the JVM is constructed with the help of figure \ref{fig:jvm}.\\
As already explained the JVM executes Java bytecode which can be in the form of *.class-files. Before the thesis dives into the structure one note has to be made: The JVM is usually delivered as a Java Runtime Environment (JRE) or Java Development Kit (JDK). These include besides the Java Virtual Machine also the Java Application Programming Interface (API) classes. These are not directly part of the JVM but play a vital role in the simplification process for programming that is undertaken by the Java project. Coming to the execution the first substructure to mention is the Class Loader Environment.\\
\begin{figure}
	\includegraphics[scale=0.2]{../figures/JVM.png}
	\caption{The Java Virtual Machine simplified structure}
	\label{fig:jvm}
\end{figure}
When talking about the JVM the mentioning of classes and interfaces is inevitable.
In further parts the distinction between class and interface is abbreviated as class.
Multiple causes for class creation exist. The class to be created can be referenced by the constant pool of another class or a class' method can be invoked via reflection.\\
The creation at the start of a program works via loading the initial class and initializing it and furthermore invoking the specified method \emph{static void main(args[])}. The complete execution is driven by this method which, as soon as the program has more than this specific method and starting class, causes loading, linking and initialization of additional classes and invoking additional methods. How do the three steps, loading, linking and initialization work? The next paragraph talks about the beginning of a class creation and loading.\\
When creating a class in execution the JVM transforms the implementation-independent bytecode into an implementation-specific internal representation of the class. The algorithm is executed step by step meaning that a class has to be completely loaded before linking can be started and it has to be verified and prepared to the full extant before initialization.\\
The loading is started by the class loader which can be the JVM bootstrap loader or a user-defined one. Before the JVM starts the class loader it checks whether the pair binary class name and class loader already exist in which case the class already exists thus eliminating the necessity of class creation. If creation proves necessary the class loader has to find the class' bytecode representation on the specific platform, usually a file with the class' name in a hierarchical filesystem. After finding, if not a specific error is thrown, the JVM parses the bytecode, which in turn might not be valid resulting in different kinds of errors. Optional superclasses may also be resolved. After these steps the pair of loader and class are saved by the JVM. Loading usually concerns the class itself however linking concerns the whole ecosystem of the respective class. \\
The JVM links a class by preparing the class, superclasses, element type in case of array classes and resolving all symbolic references which as it is a recursive algorithm includes loading and linking these as well. The two strategies for resolution are an eager algorithm, meaning immediate loading and linking on verification, or loading and linking in a lazy way meaning resources are only loaded and linked when they are about to be used. At this point the JVM knows where the binary representation of a class can be found and which other classes are in the direct ecosystem of the class but the class' structure hasn't been checked yet.\\
Checking the class' structure is the main concern of the verification phase. In this step the class is checked for a static and structural constraints list. These include checks for appropriate type usage, number of arguments, instance initialization before usage, return types and many more. Any caught error in this step lead to ta thrown VerifyError. If no such error occurs the verification is finished and acknowledges the structual correctness of the binary class representation.\\
When doing the preparation of a class the JVM checks loading constraints on methods overriding those of superclasses and create and initialize static fields to default values. This phase can occur any time between creation and initializiation but imperatively before initialization.\\
Beforehand the resolution step with two different strategies has been discussed briefly without going into detail. This omission will be rectified now. Resolution in the context of the JVM means the process of dynamically determining conrete values from symbolic references in the run-time constant pool. The target of resolution includes classes, fields, methods, method types, method handles or dynamically computed constants. These different targets require different approaches. These usually require class loading, lookup, etc. They can also fail and throw errors or succeed resulting in a resolved dynamic binding in the constant pool. With all these preceding steps the class is now ready for initialization.\\
Conceptually initializing a class is fairly simple as it is the mere execution of the initialization method. This can only be invoked by certain cases or instructions like \emph{new} or a subclass is initialized resulting in the initialization of the superclass. At this point the JVMs multithreading needs to be taken in account since synchronization is imperative. This again is solely handled by the JVM and of no concern of the application programmer. The concrete inner workings are generally spoken handled by object states and unique initialization locks. The Initialization phase is the final stage of the class loader subsystem. Now the different data areas are explained briefly.\\
The JVM run-time data area is subdivided into five different data areas. These fulfill different tasks and are also different in their relation towards processes and threads as there are data areas being defined per thread and others per process. This results in some data areas existing as long as the JVM is running and some areas' existence is linked to their respective thread's existence. The data areas are, as can be seen in figure \ref{fig:jvm}, the method area, heap, stack, program counter register and the native method stack. The next paragraphs explain which area is responsible for which task and when they are created and destroyed.\\
The method area is shared between all JVM threads and is created on JVM start-up. It stores all per-class structures like the constant pool or field and method data.\\
The heap is a JVM-defined structure, as such is created on JVM start-up, and is shared between all threads. It is an allocation space for class instances and arrays on run-time. Allocation and deallocation in the JVM is automatically handled by the garbage collector which again can be shortly defined as an automatic storage management system. The structure of the heap is not necessarily contiguous and attributes like initial, maximum and minimum size can be defined by the programmer or user.\\
The stack is a thread-defined structure and is itself subdivided into thread-specific stacks. It does have a fixed size which is defined at stack creation. Regarding the data a stack holds: local variables, partial results and has its share in method invocation and return. The not necessarily contiguous stack is only changed by pushing and popping frames.\\
The program counter (PC) register holds the current program counters of all running threads. A thread specific PC register contains the address of the thread-specific currently executed method.
The native method stack is an optional data area not supported by all JVMs. If they are supported they are conventional stacks on a thread basis. They can be of fixed or dynamic size. These stacks fulfill a similar task to the regular stacks but are for code not written in Java.\\
With the data areas explained the execution engine is the focus of the next few paragraphs.\\
The execution engine is mainly built up of three different components, the interpreter, the just-in-time (JIT) compiler and the garbage collector.\\
The interpreter is the part that executes the code directly/live including byte code getting translated to actual implementation-dependent machine code and its execution on the central processing unit (CPU) per instruction. This mechanism causes the interpreter to reinterpret, translate and execute the same code multiple times per instruction, code which is redundant work making the interpreter slow in comparison to AOT-compiled execution. When doing so the interpreter of the JVM profiles the run code for example by keeping track of how often a certain piece of code is executed. This profiling information is then used to choose the parts of code that get JIT-compiled hindering too many redundant interpretation steps.\\
A part of the byte code chosen for JIT-compilation is compiled to machine code and gets thus turned from implementation independent form into implementation dependent form. Afterwards if this part of the program gets executed again the machine code gets executed directly out from the cache where it's stored in the meantime. This method of choosing code hotspots that get executed often for compilation rather than continuous interpretation is called adaptive compilation and is used in the Oracle Hotspot VM.\\
The last component left out so far is the garbage collector (GC). It takes care of objects without references resulting them to be unreachable by application code. Such an object will be removed by GC and memory will be freed up. The GC is part of inner workings and fully functions without user interaction but the programmer can make a not guaranteed call for the GC.\cite{Lindholm}\\
This section explained the JVM ecosystem. It is split up between an AOT-compiler that creates Java bytecode out of Java source code. From there on the JVM executes the bytecode. Generally speaking the JVM consists of three areas, the class loader subsystem, taking care of loading, linking and initialization, the runtime data area and the execution engine, consisting of the interpreter, the JIT compiler and garbage collector. Aditionally to ease the programmer's work the JRE or JDK come with an API. What hasn't been discussed so far are technologies for making Java execute faster. These technologies are a core feature of GraalVM which is the topic of the next section.
\section{GraalVM}
\begin{figure}
	\includegraphics[scale=0.2]{../figures/GraalVM.png}
	\caption{The Graal Virtual Machine simplified structure}
	\label{fig:graalvm}
\end{figure}
As seen in the last two sections building a virtual machine for executing programming languages is a non-trivial task needing a lot of effort. A lot of this effort has to be redone for different programming languages. The core idea of Graal is to reduce this effort via a layered structure where each language implementation is in essence an abstract syntax tree (AST) interpreter via the Truffle API. All low level implementions like JIT compiling, data areas, memory management, optimizations and the like are reused by each language implementation connecting to the GraalVM or any JVM via Truffle. Considering performance these implementations are best run on top of GraalVM. Reasons for that are explained in the next section.\\
The GraalVM complete ecosystem is comprised of multiple technologies. In the runtime environment the GraalVM reuses most components from the HotSpot VM like the class loader subsystem, the runtime data area and parts of the execution engine, in particular the interpreter and the garbage collector, but replaces the HotSpot JIT compiler with the Graal compiler. Since the Graal compiler functions differently it also implements an interface between the regular HotSpot components and the Graal compiler. Outside of the VM it also has the Truffle framework for different language implementations. Already implemented languages are JavaScript, Ruby, R, python and LLVM-based languages like C, C++ and Rust. Thus the feature is able to end the segregation of programming languages. Another feature is its ability to be embedded in combination with OpenJDK, node.js or inside Oracle Database, while also being able to be run standalone.
\subsection{Graal Compiler}\label{sec:graalcomp}
The Graal Compiler replaces the JIT compiler in a JRE. Other as the regular java JIT compiler it doesn't translate java bytecode directly to machine code but to the Graal IR. \cite{inproceedings} The Graal IR is a graph-based high-level intermediate representation. This intermediate representation is key to Graal's aggressive optimization techniques based on optimistic assumptions such as specific types and branch prediction. Aggressive optimization also means the compilation is prone to failures which are caught by so called guards. When a guard fails execution is transferred back to interpretation. This mechanism is called deoptimization. \cite{ChambDeopt} Compiled code gets saved as a VMs internal data structure and subsequent calls to the particular part of code get always directly executed in machine code omitting the unnecessary step of interpreting.\\
Another component the GraalVM project substitutes is the Java Native Interface \cite{Lindholm}. It is exchanged with the Graal Native Function Interface (GNFI). \cite{grimmerNative} It allows for efficient native function calls within Java code.
\subsection{Truffle API}
The Truffle language implementation framework enables programmers to write an AST interpreter in Java for any programming language. The biggest advantage is the reuse of JVM runtime services such as automatic memory management with execution by a JVM. But how does the interpreter work?\\
The interpreter works by modelling language structures as nodes building the abstract syntax tree. Evaluation of the tree works by invoking the execute method which is within every type of Node. These different Node types all extend the class Node forcing them to have an execute method implementing their respective semantics. The whole tree gets then recursively evaluated by the recursive execute method calls.\\
Optimizations on AST level mostly work with a speculative specialization technique. The Nodes rewrite themselves into specialized variants at run time where profiling information is available. \cite{wuerthSelf} As we have discussed earlier in the Graal compiler speculative optimizations, like a specialization, are prone to failure. Instead of going just back to interpretation a failed specialized Node rewrites itself back into a more generic version. What kind of specializations are possible via this approach?\\
Type specialization especially with dynamic languages is an operation reducing code on runtime and thus creating speed up. To discuss this optimization method further the add method with the + operator in the scripting language JavaScript is used. The add method in JavaScript is defined for various types including Strings and Numbers. We reduce the language's complexity to the forementioned two types in the example.\\
\begin{lstlisting}[caption=JavaScript addition types minimal example]
	var w = 1 + "Test" // results in "1Test"
	typeof(w) // string
	var x = 1 + 2 + "Test" // results in "3Test"
	typeof(x) // string
	var y = +"1" + 2 // results in 3
	typeof(y) // number
	var z = +"Test" + 1 // results in NaN
	typeof(z) // number
\end{lstlisting}
The above example code shows how addition in JavaScript works. The interpreter has to check the types of two operands from left to right before it can decide on which operation to use, an addition for numbers or one for strings.\\
\begin{figure}
	\includegraphics[scale=0.165]{../figures/NodeJIT.png}
	\caption{Simplified Node Rewriting example \cite{VMRule}}
	\label{fig:trufNR}
\end{figure}
The node rewriting process shown in figure \ref{fig:trufNR} happens at various state to achieve different goals. In the interpreter node rewriting is used to capture profiling feedback via runtime information such as the rate of node rewrites. If certain thresholds are surpassed the specialized AST gets compiled into machine code. If the execution fails the compiled code gets deoptimized back to the AST interpreter where again nodes are rewritten and now update the existing profile information. After this step the newly specialized AST gets compiled again.\\
One note on the compilation process: The compiler uses as said aggressive optimization methods. This operation is successful for stable ASTs. Then compilation techniques like method inlining and eliminating interpreter dispatch code can be performed. This technique is called partial evaluation.\\
In addition to type specialization truffle offers so called polymorphic inline caches. \cite{ChambOpti} An inline cache itself is simply caching a method from a former lookup at call site. In Truffle inline caches are built by chaining nodes which check for cached target matches which are then executed. At a predefined chain length the chain gets replaced by a polymorphic node handling all operations.\\
As discussed in the JVM section program loading often requires resolution. An implemented truffle language can cache resolved targets at run time by replacing an unresolved node with its resolved version. Thus further resolutions of this node are prevented which leads to an optimized version of the AST.\\
This section gave a small introduction into the Truffle API for implementing languages via an AST interpreter for running on top of a JVM and showed the optimizations that are conducted on AST level. After an AST at run time becomes stable with no more rewrites the AST is compiled dynamically to machine code. Truffle uses the JIT compiler of a JVM, in case of GraalVM the Graal compiler. If Graal is used the nodes' execute methods are inlined producing one compilation unit of the whole tree. In the next step the Graal compiler uses its aggressive optimization techniques for the whole inlined tree unit producing efficient machine code. These combined methods are based on the principle of the first Futamura Projection also called partial evaluation. \cite{FutaPart} As stated in section \ref{sec:graalcomp} guards or also optimization points need to be implemented to check all the assumptions made before compilation. As soon as it is necessary to rewrite a node during machine code execution the control flow gets transferred back to the interpreted AST to rewrite the node either on profiling information to another specialized version or a more generic version. \cite{ChambDebug}

In conclusion the Graal project aims at simplifying language implementations and application programming while also trying to have a performant run time. The simplification comes from introducing a framework for implementing languages as AST interpreters which can run on top of any JVM. The performance comes especially from running the AST interpreter on top of the GraalVM which uses various techniques to bring run time performance to higher levels.
%\begin{tikzpicture}[nodes={draw, circle}, ->, minimum size=2cm, level distance = 2.5cm, sibling distance = 2.5cm]
%	\node[fill=lightorange] (p) {U(+)}
%		child { node[fill=lightorange] (n) {U(1)}}
%		child { node[fill=lightorange] (s) {U("Test")}};
%\end{tikzpicture}
%\begin{tikzpicture}[nodes={draw, circle}, ->, minimum size=2cm, level distance = 2.5cm, sibling distance = 2.5cm]
%\node[fill=lightgreen] (sp) {S(+)}
%child {node[fill=lightblue] (sn) {N(1)}}
%child {node[fill=lightgreen] (ss) {S("Test")}};
%\end{tikzpicture}\\ \\
%\begin{tikzpicture}[sibling distance=1cm]
%\node[fill=lightorange, label=right:{Uninitialized}] (u) {U};
%\node[below=.3cm of u,fill=lightblue, label=right:{Number}] (n) {N};
%\end{tikzpicture}
\section{Graal.Js}

\section{ECMAScript modules}
\subsection{Current State}

\subsection{Proposal: Module blocks}